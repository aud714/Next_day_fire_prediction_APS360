{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ft8RZjvZLHTN","cellView":"form"},"outputs":[],"source":[" #@title Modules\n","\n","import re\n","from typing import Dict, List, Optional, Text, Tuple\n","import os\n","import time\n","from time import sleep\n","from tqdm import tqdm, trange\n","from scipy.ndimage import zoom\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","from matplotlib.colors import LinearSegmentedColormap\n","from PIL import Image\n","import cv2\n","\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import Dataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset\n","from torchvision import datasets, models, transforms\n","import torchvision.models\n","from torchvision.transforms.functional import F_pil"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15438,"status":"ok","timestamp":1691636114103,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"},"user_tz":240},"id":"9kwK3htwL-Nu","outputId":"c9fd42ac-d9ca-4a47-f656-eaa9b7a62708"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#@title Mount Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"NEJWO4FmLQlt"},"source":["## Load Data Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lUgEnEheMMTA"},"outputs":[],"source":["#@title File Path\n","\n","train_00_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_train_00.tfrecord'\n","train_01_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_train_01.tfrecord'\n","train_02_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_train_02.tfrecord'\n","train_03_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_train_03.tfrecord'\n","train_04_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_train_04.tfrecord'\n","train_05_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_train_05.tfrecord'\n","train_06_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_train_06.tfrecord'\n","\n","val_00_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_eval_00.tfrecord'\n","\n","test_02_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_test_02.tfrecord'"]},{"cell_type":"code","source":["test_unseen_01_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_test_unseen_01.tfrecord'\n","test_unseen_02_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_test_unseen_02.tfrecord'\n","test_unseen_03_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/next_day_wildfire_spread_test_unseen_03.tfrecord'"],"metadata":{"id":"Aov1XnO41Ewa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeBQvdBiMQXL","cellView":"form"},"outputs":[],"source":["#@title Data Information & Statistic\n","\n","\"\"\"Constants for the data reader.\"\"\"\n","\n","INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n","                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n","\n","OUTPUT_FEATURES = ['FireMask', ]\n","\n","# Data statistics\n","# For each variable, the statistics are ordered in the form:\n","# (min_clip, max_clip, mean, standard deviation)\n","DATA_STATS = {\n","    # Elevation in m.\n","    # 0.1 percentile, 99.9 percentile\n","    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n","\n","    # Drought Index (Palmer Drought Severity Index)\n","    # 0.1 percentile, 99.9 percentile\n","    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n","\n","    #Vegetation index (times 10,000 maybe, since it's supposed to be b/w -1 and 1?)\n","    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n","\n","    # Precipitation in mm.\n","    # Negative values do not make sense, so min is set to 0.\n","    # 0., 99.9 percentile\n","    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n","\n","    # Specific humidity.\n","    # Negative values do not make sense, so min is set to 0.\n","    # The range of specific humidity is up to 100% so max is 1.\n","    'sph': (0., 1., 0.0071658953, 0.0042835088),\n","\n","    # Wind direction in degrees clockwise from north.\n","    # Thus min set to 0 and max set to 360.\n","    'th': (0., 360.0, 190.32976, 72.59854),\n","\n","    # Min/max temperature in Kelvin.\n","\n","    #Min temp\n","    # -20 degree C, 99.9 percentile\n","    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n","\n","    #Max temp\n","    # -20 degree C, 99.9 percentile\n","    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n","\n","    # Wind speed in m/s.\n","    # Negative values do not make sense, given there is a wind direction.\n","    # 0., 99.9 percentile\n","    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n","\n","    # NFDRS fire danger index energy release component expressed in BTU's per\n","    # square foot.\n","    # Negative values do not make sense. Thus min set to zero.\n","    # 0., 99.9 percentile\n","    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n","\n","    # Population density\n","    # min, 99.9 percentile\n","    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n","\n","    # We don't want to normalize the FireMasks.\n","    # 1 indicates fire, 0 no fire, -1 unlabeled data\n","    'PrevFireMask': (-1., 1., 0., 1.),\n","    'FireMask': (-1., 1., 0., 1.)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6oc0NadMh17","cellView":"form"},"outputs":[],"source":["#@title Cropping function & Data Reader\n","\n","\"\"\"Library of common functions used in deep learning neural networks.\n","\"\"\"\n","#YOU PROBABLY WILL NOT USE THESE.\n","\n","def random_crop_input_and_output_images(\n","    input_img: tf.Tensor,\n","    output_img: tf.Tensor,\n","    sample_size: int,\n","    num_in_channels: int,\n","    num_out_channels: int,\n",") -> Tuple[tf.Tensor, tf.Tensor]:\n","  \"\"\"Randomly axis-align crop input and output image tensors.\n","\n","  Args:\n","    input_img: tensor with dimensions HWC.\n","    output_img: tensor with dimensions HWC.\n","    sample_size: side length (square) to crop to.\n","    num_in_channels: number of channels in input_img.\n","    num_out_channels: number of channels in output_img.\n","  Returns:\n","    input_img: tensor with dimensions HWC.\n","    output_img: tensor with dimensions HWC.\n","  \"\"\"\n","  combined = tf.concat([input_img, output_img], axis=2)\n","  combined = tf.image.random_crop(\n","      combined,\n","      [sample_size, sample_size, num_in_channels + num_out_channels])\n","  input_img = combined[:, :, 0:num_in_channels]\n","  output_img = combined[:, :, -num_out_channels:]\n","  return input_img, output_img\n","\n","\n","def center_crop_input_and_output_images(\n","    input_img: tf.Tensor,\n","    output_img: tf.Tensor,\n","    sample_size: int,\n",") -> Tuple[tf.Tensor, tf.Tensor]:\n","  \"\"\"Center crops input and output image tensors.\n","\n","  Args:\n","    input_img: tensor with dimensions HWC.\n","    output_img: tensor with dimensions HWC.\n","    sample_size: side length (square) to crop to.\n","  Returns:\n","    input_img: tensor with dimensions HWC.\n","    output_img: tensor with dimensions HWC.\n","  \"\"\"\n","  central_fraction = sample_size / input_img.shape[0]\n","  input_img = tf.image.central_crop(input_img, central_fraction)\n","  output_img = tf.image.central_crop(output_img, central_fraction)\n","  return input_img, output_img\n","\n","\"\"\"Dataset reader for Earth Engine data.\"\"\"\n","\n","def _get_base_key(key: Text) -> Text:\n","  \"\"\"Extracts the base key from the provided key.\n","\n","  Earth Engine exports TFRecords containing each data variable with its\n","  corresponding variable name. In the case of time sequences, the name of the\n","  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n","  where 'variable' is the name of the variable, and n the number of elements\n","  in the time sequence. Extracting the base key ensures that each step of the\n","  time sequence goes through the same normalization steps.\n","  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n","  For instance, for an input key 'variable_1', this function returns 'variable'.\n","  For an input key 'variable', this function simply returns 'variable'.\n","\n","  Args:\n","    key: Input key.\n","\n","  Returns:\n","    The corresponding base key.\n","\n","  Raises:\n","    ValueError when `key` does not match the expected pattern.\n","  \"\"\"\n","  match = re.match(r'([a-zA-Z]+)', key)\n","  if match:\n","    return match.group(1)\n","  raise ValueError(\n","      'The provided key does not match the expected pattern: {}'.format(key))\n","\n","\n","def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n","  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n","\n","  Args:\n","    inputs: Inputs to clip and rescale.\n","    key: Key describing the inputs.\n","\n","  Returns:\n","    Clipped and rescaled input.\n","\n","  Raises:\n","    ValueError if there are no data statistics available for `key`.\n","  \"\"\"\n","  base_key = _get_base_key(key)\n","  if base_key not in DATA_STATS:\n","    raise ValueError(\n","        'No data statistics available for the requested key: {}.'.format(key))\n","  min_val, max_val, _, _ = DATA_STATS[base_key]\n","  inputs = tf.clip_by_value(inputs, min_val, max_val)\n","  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n","\n","\n","def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n","  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n","\n","  Args:\n","    inputs: Inputs to clip and normalize.\n","    key: Key describing the inputs.\n","\n","  Returns:\n","    Clipped and normalized input.\n","\n","  Raises:\n","    ValueError if there are no data statistics available for `key`.\n","  \"\"\"\n","  base_key = _get_base_key(key)\n","  if base_key not in DATA_STATS:\n","    raise ValueError(\n","        'No data statistics available for the requested key: {}.'.format(key))\n","  min_val, max_val, mean, std = DATA_STATS[base_key]\n","  inputs = tf.clip_by_value(inputs, min_val, max_val)\n","  inputs = inputs - mean\n","  return tf.math.divide_no_nan(inputs, std)\n","\n","def _get_features_dict(\n","    sample_size: int,\n","    features: List[Text],\n",") -> Dict[Text, tf.io.FixedLenFeature]:\n","  \"\"\"Creates a features dictionary for TensorFlow IO.\n","\n","  Args:\n","    sample_size: Size of the input tiles (square).\n","    features: List of feature names.\n","\n","  Returns:\n","    A features dictionary for TensorFlow IO.\n","  \"\"\"\n","  sample_shape = [sample_size, sample_size]\n","  features = set(features)\n","  columns = [\n","      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n","      for _ in features\n","  ]\n","  return dict(zip(features, columns))\n","\n","\n","def _parse_fn(\n","    example_proto: tf.train.Example, data_size: int, sample_size: int,\n","    num_in_channels: int, clip_and_normalize: bool,\n","    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",") -> Tuple[tf.Tensor, tf.Tensor]:\n","  \"\"\"Reads a serialized example.\n","\n","  Args:\n","    example_proto: A TensorFlow example protobuf.\n","    data_size: Size of tiles (square) as read from input files.\n","    sample_size: Size the tiles (square) when input into the model.\n","    num_in_channels: Number of input channels.\n","    clip_and_normalize: True if the data should be clipped and normalized.\n","    clip_and_rescale: True if the data should be clipped and rescaled.\n","    random_crop: True if the data should be randomly cropped.\n","    center_crop: True if the data should be cropped in the center.\n","\n","  Returns:\n","    (input_img, output_img) tuple of inputs and outputs to the ML model.\n","  \"\"\"\n","  if (random_crop and center_crop):\n","    raise ValueError('Cannot have both random_crop and center_crop be True')\n","  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n","  feature_names = input_features + output_features\n","  features_dict = _get_features_dict(data_size, feature_names)\n","  features = tf.io.parse_single_example(example_proto, features_dict)\n","\n","  if clip_and_normalize:\n","    inputs_list = [\n","        _clip_and_normalize(features.get(key), key) for key in input_features\n","    ]\n","  elif clip_and_rescale:\n","    inputs_list = [\n","        _clip_and_rescale(features.get(key), key) for key in input_features\n","    ]\n","  else:\n","    inputs_list = [features.get(key) for key in input_features]\n","\n","  inputs_stacked = tf.stack(inputs_list, axis=0)\n","  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n","\n","  outputs_list = [features.get(key) for key in output_features]\n","  assert outputs_list, 'outputs_list should not be empty'\n","  outputs_stacked = tf.stack(outputs_list, axis=0)\n","\n","  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n","  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n","                                            'but dimensions of outputs_stacked'\n","                                            f' are {outputs_stacked_shape}')\n","  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n","\n","  if random_crop:\n","    input_img, output_img = random_crop_input_and_output_images(\n","        input_img, output_img, sample_size, num_in_channels, 1)\n","  if center_crop:\n","    input_img, output_img = center_crop_input_and_output_images(\n","        input_img, output_img, sample_size)\n","  return input_img, output_img\n","\n","\n","def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n","                batch_size: int, num_in_channels: int, compression_type: Text,\n","                clip_and_normalize: bool, clip_and_rescale: bool,\n","                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n","  \"\"\"Gets the dataset from the file pattern.\n","\n","  Args:\n","    file_pattern: Input file pattern.\n","    data_size: Size of tiles (square) as read from input files.\n","    sample_size: Size the tiles (square) when input into the model.\n","    batch_size: Batch size.\n","    num_in_channels: Number of input channels.\n","    compression_type: Type of compression used for the input files.\n","    clip_and_normalize: True if the data should be clipped and normalized, False\n","      otherwise.\n","    clip_and_rescale: True if the data should be clipped and rescaled, False\n","      otherwise.\n","    random_crop: True if the data should be randomly cropped.\n","    center_crop: True if the data shoulde be cropped in the center.\n","\n","  Returns:\n","    A TensorFlow dataset loaded from the input file pattern, with features\n","    described in the constants, and with the shapes determined from the input\n","    parameters to this function.\n","  \"\"\"\n","  if (clip_and_normalize and clip_and_rescale):\n","    raise ValueError('Cannot have both normalize and rescale.')\n","  dataset = tf.data.Dataset.list_files(file_pattern)\n","  dataset = dataset.interleave(\n","      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","  dataset = dataset.map(\n","      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n","          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n","          clip_and_rescale, random_crop, center_crop),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","  return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8bvVCgAMzfm","cellView":"form"},"outputs":[],"source":["#@title Avg Neighbors\n","\n","#Eventually would like a function that takes in an input array of dimensions nxn,\n","#outputs an array that gives avg of each cell's neighbors:\n","def avg_neighbors(array_in):\n","    #Check input\n","    if array_in.shape[0] != array_in.shape[1]:\n","        raise Exception('Only square arrays make sense here, since you\\'re analyzing square arrays.')\n","    #Maybe should also do type-checking, but leave it for now.\n","\n","    #Prepare the output array:\n","    n = array_in.shape[0]\n","    array_out = np.zeros((n,n))\n","\n","    #Guess who doesn't know how to do signal processing in Python...\n","\n","    for i in range(n):\n","        for j in range(n):\n","            if i == 0:\n","                #Upper edge\n","                if j == 0:\n","                    #Upper left corner\n","                    sum_neighbors = array_in[i+1, j] + array_in[i, j+1] + array_in[i+1, j+1]\n","                    avg = sum_neighbors/3\n","\n","                elif j == (n-1):\n","                    #Upper right corner\n","                    sum_neighbors = array_in[i, j-1] + array_in[i+1, j] + array_in[i+1, j-1]\n","                    avg = sum_neighbors/3\n","\n","                else:\n","                    #Upper edge except corners\n","                    sum_neighbors = array_in[i, j-1] + array_in[i+1, j] + array_in[i, j+1] + array_in[i+1, j-1] + array_in[i+1, j+1]\n","                    avg = sum_neighbors/5\n","\n","            elif i == (n-1):\n","                #Lower edge\n","                if j == 0:\n","                    #Lower left corner\n","                    sum_neighbors = array_in[i-1, j] + array_in[i, j+1] + array_in[i-1, j+1]\n","                    avg = sum_neighbors/3\n","\n","                elif j == (n-1):\n","                    #Lower right corner\n","                    sum_neighbors = array_in[i, j-1] + array_in[i-1, j] + array_in[i-1, j-1]\n","                    avg = sum_neighbors/3\n","\n","                else:\n","                    #Lower edge except corners\n","                    sum_neighbors = array_in[i, j-1] + array_in[i, j+1] + array_in[i-1, j] + array_in[i-1, j-1] + array_in[i-1, j+1]\n","                    avg = sum_neighbors/5\n","\n","            else:\n","                if j == 0:\n","                    #Left edge except corners\n","                    sum_neighbors = array_in[i-1, j] + array_in[i+1, j] + array_in[i, j+1] + array_in[i-1, j+1] + array_in[i+1, j+1]\n","                    avg = sum_neighbors/5\n","\n","                elif j == (n-1):\n","                    #Right edge except corners\n","                    sum_neighbors = array_in[i-1, j] + array_in[i+1, j] + array_in[i, j-1] + array_in[i-1, j-1] + array_in[i+1, j-1]\n","                    avg = sum_neighbors/5\n","\n","                else:\n","                    #Not on any edge or corner\n","                    sum_neighbors = array_in[i, j+1] + array_in[i, j-1] + array_in[i-1, j] + array_in[i+1, j] + \\\n","                                    array_in[i-1, j-1] + array_in[i-1, j+1] + array_in[i+1, j-1] + array_in[i+1, j+1]\n","                    avg = sum_neighbors/8\n","\n","\n","            array_out[i,j] = avg\n","            #/for loop body\n","\n","    return array_out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyrnG7LbNU7a","cellView":"form"},"outputs":[],"source":["#@title Elim_Uncertain, Extract Certain Label, Avg Neighbors in Batch\n","\n","#Function to identify certain observations in the previous fire mask and return:\n","#1) a list of their indices in the batch (need this to grab the right ones from the labels) and\n","#2) the actual array of certain observations\n","\n","def elim_uncertain(prev_fire_mask_batch):\n","\n","    prev_masks_array = np.array(prev_fire_mask_batch)\n","    num_imgs, rows, cols = prev_masks_array.shape\n","\n","    #Build the array of certain data AND SAVE THE INDICES\n","    first_find_flag = 1\n","    count = 0\n","    indices = []\n","\n","    for img_num in range(num_imgs):\n","        fire_mask = prev_fire_mask_batch[img_num, :, :] #grab the \"working fire mask\" off the pile\n","\n","        if (np.all( np.invert(fire_mask == -1) )): #If no missing data, condition is TRUE.\n","            count += 1\n","            indices.append(img_num)\n","            if first_find_flag == 1: #If you need to start the array\n","                certain_prev_fire_masks_batch = fire_mask\n","                first_find_flag = 0  #Remember to turn the flag off!\n","            else:\n","                certain_prev_fire_masks_batch = np.dstack((certain_prev_fire_masks_batch, fire_mask))\n","\n","    return certain_prev_fire_masks_batch, indices\n","\n","#Function to extract only the labels (i.e. current fire masks) from the certain observations\n","def extract_certain_labels(certain_indices, og_labels):\n","\n","    for i, index in enumerate(certain_indices):\n","        if i == 0:\n","            extracted_labels = og_labels[index,:,:,:] #the og_labels dimensions are batch_size by sidelength by sidelength by 1\n","        else:\n","            #labels\n","            extracted_labels = np.concatenate((extracted_labels, og_labels[index,:,:,:]), axis=2)\n","\n","    return extracted_labels\n","\n","#Create multi-D array of neighbor fire values\n","def avg_neighbor_batch(batch_in):\n","    rows, cols, batch_size = batch_in.shape #ordering of dimensions here meant to be compatible with elim_uncertain and extract_certain_labels\n","    batch_out = np.zeros((rows, cols, batch_size))\n","    for i in range(batch_size):\n","        working_arr = batch_in[:,:,i]\n","        avgd_arr = avg_neighbors(working_arr)\n","        batch_out[:,:,i] =  avgd_arr\n","    #/for loop\n","\n","    return batch_out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d95-Q9ted5u8","cellView":"form"},"outputs":[],"source":["#@title extract_data\n","\n","def extract_data(file_path, batch_num, image_dim=64, cropping=False, average_neighbors=False):\n","  '''\n","  input:\n","    file_path, path of tfrecord\n","    batch_num, batch size\n","    image_dim, image dimensiion; at most 64\n","    cropping, whether or not to crop\n","    average_neighbors, whether or not to average the fire\n","\n","  takes in the input with specified task, pass it through functions that eliminates uncertainties\n","\n","  Output:\n","    good_prev_masks, array of relevent prev fire masks\n","    good_labels, array of labels corresponding to the relevent prevfire masks\n","    good_indices, indices of relvent fire masks in the batch\n","    inputs, all features of the dataset\n","\n","  '''\n","  dataset = get_dataset(\n","      file_path,\n","      data_size=64,\n","      sample_size=image_dim,\n","      batch_size=batch_num,\n","      num_in_channels=12,\n","      compression_type=None,\n","      clip_and_normalize=False,\n","      clip_and_rescale=False,\n","      random_crop=cropping,\n","      center_crop=False)\n","\n","  inputs, labels = next(iter(dataset))\n","  # inputs.shape --> (batch_size, img_dim, img_dim, feature_col)\n","  # labels.shape --> (batch_size, img_dim, img_dim, 1)\n","\n","  all_prev_fire_masks = np.array(inputs[:,:,:,11]) # shape --> (batch_size, img_dim, img_dim)\n","  all_curr_fire_masks = np.array(labels)\n","\n","  good_prev_masks, good_indices = elim_uncertain(all_prev_fire_masks)\n","  # good_prev_masks.shape --> (img_dim, img_dim, batch_size)\n","  # good_indices, a list of the index of good prev fire masks, indexed base on batch number\n","\n","  good_labels = extract_certain_labels(good_indices, all_curr_fire_masks)\n","  # good_labels.shape --> (img_dim, img_dim, batch_size)\n","  # good prev fire masks' corresponding label, compiled in an array\n","\n","  if average_neighbors: # if average_neighbors is set to true, average function will run\n","    good_prev_masks = avg_neighbor_batch(good_prev_masks)\n","    # outputs retains the same shape as input but pixels are average\n","\n","  return good_prev_masks, good_labels, good_indices, inputs\n"]},{"cell_type":"markdown","metadata":{"id":"Y8ypCEw-xXAg"},"source":["## Load and Save TFRecord Data as JPG Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_ZtKlMe5maj","cellView":"form"},"outputs":[],"source":["#@title Combine good masks, label with their other corresponding features\n","\n","# location on Google Drive\n","master_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project'\n","\n","# Load data\n","batch_size = 1 #want to save 1 file at a time -> batch_size = 1\n","\n","#convert tfrecord file into 1 big array of only good data\n","def add_to_array(data_path):\n","    prev_masks, good_labels, good_indices, all_inputs = extract_data(data_path, 100)\n","    all_inputs = np.array(all_inputs)\n","    for i in range(len(good_indices)):\n","        prev_mask = prev_masks[:,:,i] #get the single prev_mask in the correct order\n","        prev_mask = prev_mask[..., np.newaxis] #convert prev_mask to 3D array\n","        good_label = good_labels[:,:,i] #get the single good_label in the correct order\n","        good_label = good_label[..., np.newaxis] #convert good_label to 3D array\n","        last_two_labels = np.concatenate((prev_mask, good_label), axis=2) #combine prev_mask and good_label into 1 array\n","\n","        if i == 0:\n","          big_arr = np.concatenate((all_inputs[good_indices[i], :, :, 0:11], last_two_labels), axis=2) #store as first array in big_arr\n","          big_arr = big_arr[np.newaxis, ...] #\n","        else:\n","          new_arr = np.concatenate((all_inputs[good_indices[i], :, :, 0:11], last_two_labels), axis=2) #add to first array\n","          new_arr = new_arr[np.newaxis, ...]\n","          big_arr = np.concatenate((big_arr, new_arr), axis=0)\n","    return big_arr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zbjQ8Lvl9R4U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691636180901,"user_tz":240,"elapsed":1764,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"4c1767bd-7d3e-48d2-9140-e4fb62bf10ef","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 64)\n","(64, 64, 11)\n","(64, 64, 2)\n"]},{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":11}],"source":["#@title Testing the code above\n","data_path = train_00_path\n","prev_masks, good_labels, good_indices, all_inputs = extract_data(data_path, 100)\n","all_inputs = np.array(all_inputs)\n","print(prev_masks[:,:,0].shape)\n","print(all_inputs[good_indices[0], :, :, 0:11].shape)\n","var = prev_masks[:,:,1]\n","var = var[..., np.newaxis]\n","var.shape\n","prev_mask = prev_masks[:,:,0] #get the single prev_mask in the correct order\n","prev_mask = prev_mask[..., np.newaxis] #convert prev_mask to 3D array\n","good_label = good_labels[:,:,0] #get the single good_label in the correct order\n","good_label = good_label[..., np.newaxis] #convert good_label to 3D array\n","last_two_labels = np.concatenate((prev_mask, good_label), axis=2) #combine prev_mask and good_label into 1 array\n","print(last_two_labels.shape)\n","big_arr = np.concatenate((all_inputs[good_indices[0], :, :, 0:11], last_two_labels), axis=2) #store as first array in big_arr\n","big_arr = big_arr[np.newaxis, ...]\n","new_arr = np.concatenate((all_inputs[good_indices[1], :, :, 0:11], last_two_labels), axis=2) #add to first array\n","new_arr = new_arr[np.newaxis, ...]\n","big_arr = np.concatenate((big_arr, new_arr), axis=0)\n","big_arr.shape\n","big_arr.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JurA1iiv6yW7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691643061289,"user_tz":240,"elapsed":9019,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"b9836df4-dbd9-474e-cafd-d1a46e02badf","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["(483, 64, 64, 13)\n","(73, 64, 64, 13)\n"]}],"source":["#@title Forming np array of all training data\n","train_arr1 = add_to_array(train_00_path)\n","train_arr2 = add_to_array(train_01_path)\n","train_arr3 = add_to_array(train_02_path)\n","train_arr4 = add_to_array(train_03_path)\n","train_arr5 = add_to_array(train_04_path)\n","train_arr6 = add_to_array(train_05_path)\n","train_arr7 = add_to_array(train_06_path)\n","whole_train_arr = np.concatenate((train_arr1, train_arr2), axis=0)\n","whole_train_arr = np.concatenate((whole_train_arr, train_arr3), axis=0)\n","whole_train_arr = np.concatenate((whole_train_arr, train_arr4), axis=0)\n","whole_train_arr = np.concatenate((whole_train_arr, train_arr5), axis=0)\n","whole_train_arr = np.concatenate((whole_train_arr, train_arr6), axis=0)\n","whole_train_arr = np.concatenate((whole_train_arr, train_arr7), axis=0)\n","print(whole_train_arr.shape)\n","\n","#forming np array of val data\n","val_arr = add_to_array(val_00_path)\n","print(val_arr.shape)"]},{"cell_type":"code","source":["#forming np array of all test data\n","test_unseen_arr1 = add_to_array(test_unseen_01_path)\n","test_unseen_arr2 = add_to_array(test_unseen_02_path)\n","test_unseen_arr3 = add_to_array(test_unseen_03_path)\n","test_unseen_arr4 = add_to_array(test_02_path)\n","whole_test_unseen_arr = np.concatenate((test_unseen_arr1, test_unseen_arr2), axis=0)\n","whole_test_unseen_arr = np.concatenate((whole_test_unseen_arr, test_unseen_arr3), axis=0)\n","whole_test_unseen_arr = np.concatenate((whole_test_unseen_arr, test_unseen_arr4), axis=0)"],"metadata":{"id":"PsJaMIu71Q8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(whole_test_unseen_arr.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Mj-G95NYHuj","executionInfo":{"status":"ok","timestamp":1691643118279,"user_tz":240,"elapsed":108,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"df686ce2-d787-4529-c0d8-2ea9fff3ee7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(295, 64, 64, 13)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzL9tu9Gxgns","cellView":"form"},"outputs":[],"source":["#@title Upscaling and Saving Images to Folder\n","def features_to_folder(set_name, folder_path, data_arr):\n","    for n in range(data_arr.shape[0]):\n","      for i in range(data_arr.shape[3]):\n","        #save each feature to respective data folder with the right name\n","        folder_name = folder_path + \"/Input\" + '/' + set_name + '/data_' + str(n)\n","        if not os.path.isdir(folder_name):\n","          os.mkdir(folder_name)\n","\n","        upscaled_img = zoom(whole_train_arr[n,:,:,i], 3.5, order=0)\n","\n","        if i <= 10: #if is not a mask feature, save w cmap = 'viridis'\n","              plt.imsave(folder_name + '/feature_' + str(i) + '.jpg', upscaled_img, cmap = 'viridis')\n","        elif i == 11: #else hv to save mask as rgb img\n","              CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n","              BOUNDS = [-1, -0.1, 0.001, 1]\n","              NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n","              plt.imsave(folder_name + '/feature_' + str(i) + '.jpg', upscaled_img, cmap=CMAP)\n","        elif i == 12: #else if it's the label, save as rgb img w/ name \"label\"\n","              #label_img = whole_train_arr[n,:,:,i]\n","              CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n","              BOUNDS = [-1, -0.1, 0.001, 1]\n","              NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n","              plt.imsave(folder_name + '/label.jpg', upscaled_img, cmap=CMAP)"]},{"cell_type":"code","source":["features_to_folder(\"train\", master_path, whole_train_arr)\n","features_to_folder(\"val\", master_path, val_arr)\n","#features_to_folder(\"test\", master_path, test_arr)"],"metadata":{"id":"wmZd1_Mp1W88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_to_folder(\"test_unseen\", master_path, whole_test_unseen_arr)"],"metadata":{"id":"EERGW-qH1awV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clean gray"],"metadata":{"id":"HD8mKhDLSZI4"}},{"cell_type":"code","source":["def preclean_input_loader(purpose):\n","\n","  output = []\n","\n","  folder_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/Input'\n","  folder_path = folder_path + '/' + purpose\n","  data_dir = os.listdir(folder_path)\n","\n","  for i in range(len(data_dir)):\n","    data_path = os.path.join(folder_path, data_dir[i])\n","    feature_dir = os.listdir(data_path)\n","\n","    data = []\n","    label = []\n","\n","    for j in range(len(feature_dir)):\n","      feature_path = os.path.join(data_path, feature_dir[j])\n","\n","      print(feature_path)\n","\n","      if j == 11:\n","          data = Image.open(feature_path)\n","          data = np.array(data)\n","\n","      elif j == 12:\n","          label_path = feature_path\n","          label = Image.open(label_path)\n","          label = np.array(label)\n","\n","    output.append((data, label))\n","\n","  return output"],"metadata":{"id":"qOic6YogM13y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preclean_train = preclean_input_loader('train')\n","preclean_val = preclean_input_loader('val')\n","#preclean_test = preclean_input_loader('test')"],"metadata":{"id":"qhyT5NgAR5WI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preclean_test_unseen = preclean_input_loader('test_unseen')"],"metadata":{"id":"KhCLEa6S1hPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Function:\n","#for data, label in each folder\n","#find images with gray pixels (192, 192, 192)\n","#if either data or label contains gray pixels, os.remove the corresponding folder\n","\n","def clean_grey(data_loader, set_name):\n","  count = 0\n","  num = 0\n","  folder_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/Input'\n","  for data, label in data_loader:\n","    if ([192, 192, 192] in data) or ([192, 192, 192] in label):\n","      data_grey = np.count_nonzero(data==192)\n","      label_grey = np.count_nonzero(label==192)\n","      if (data_grey>50) or (label_grey>50):\n","        print(\"File\", count, \"data_grey:\", data_grey, \"label_grey:\", label_grey)\n","        grey_direct = folder_path + '/' + set_name + '/data_' + str(count)\n","\n","        for root, dirs, files in os.walk(grey_direct):\n","        # For each file in the directory\n","          for file in files:\n","            # Construct the full path to the file\n","            file_path = os.path.join(root, file)\n","            # Delete the file\n","            os.remove(file_path)\n","            # For each subdirectory in the directory\n","          for dir in dirs:\n","            # Construct the full path to the subdirectory\n","            dir_path = os.path.join(root, dir)\n","            # Delete the subdirectory\n","            os.rmdir(dir_path)\n","        # Delete the top-level directory\n","        os.rmdir(grey_direct)\n","\n","        num += 1\n","    count += 1\n","  print(\"Number of file with grey pixels:\", num)"],"metadata":{"id":"eay_Z4zrUZxe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clean_grey(preclean_train, 'train')\n","clean_grey(preclean_val, 'val')\n","#clean_grey(preclean_test, 'test')"],"metadata":{"id":"X7jMM9y2VlQe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691650819106,"user_tz":240,"elapsed":5398,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"07446c41-5e61-4545-d1dc-9a1a539e1c07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File 11 data_grey: 4 label_grey: 147579\n","File 16 data_grey: 0 label_grey: 142157\n","File 18 data_grey: 0 label_grey: 143389\n","File 40 data_grey: 0 label_grey: 141449\n","File 57 data_grey: 0 label_grey: 148678\n","File 67 data_grey: 1 label_grey: 129017\n","File 75 data_grey: 0 label_grey: 148393\n","File 76 data_grey: 1 label_grey: 142226\n","File 79 data_grey: 1 label_grey: 144768\n","File 83 data_grey: 2 label_grey: 135293\n","File 87 data_grey: 0 label_grey: 144248\n","File 101 data_grey: 2 label_grey: 140036\n","File 103 data_grey: 1 label_grey: 144712\n","File 110 data_grey: 0 label_grey: 142628\n","File 122 data_grey: 1 label_grey: 112381\n","File 129 data_grey: 0 label_grey: 136535\n","File 132 data_grey: 4 label_grey: 132414\n","File 134 data_grey: 1 label_grey: 140372\n","File 135 data_grey: 0 label_grey: 143880\n","File 136 data_grey: 5 label_grey: 141581\n","File 137 data_grey: 0 label_grey: 138225\n","File 138 data_grey: 2 label_grey: 142153\n","File 141 data_grey: 0 label_grey: 144772\n","File 145 data_grey: 0 label_grey: 144965\n","File 147 data_grey: 1 label_grey: 143536\n","File 184 data_grey: 5 label_grey: 126131\n","File 195 data_grey: 0 label_grey: 147684\n","File 197 data_grey: 0 label_grey: 148555\n","File 198 data_grey: 0 label_grey: 145165\n","File 205 data_grey: 1 label_grey: 148045\n","File 206 data_grey: 0 label_grey: 139771\n","File 220 data_grey: 0 label_grey: 143623\n","File 222 data_grey: 0 label_grey: 147682\n","File 223 data_grey: 2 label_grey: 139512\n","File 225 data_grey: 1 label_grey: 147834\n","File 230 data_grey: 1 label_grey: 143895\n","File 234 data_grey: 4 label_grey: 140376\n","File 236 data_grey: 0 label_grey: 146323\n","File 244 data_grey: 0 label_grey: 147885\n","File 245 data_grey: 0 label_grey: 120314\n","File 248 data_grey: 8 label_grey: 113043\n","File 251 data_grey: 1 label_grey: 131568\n","File 258 data_grey: 2 label_grey: 141464\n","File 259 data_grey: 2 label_grey: 130533\n","File 262 data_grey: 0 label_grey: 141920\n","File 263 data_grey: 15 label_grey: 141123\n","File 265 data_grey: 4 label_grey: 139654\n","File 267 data_grey: 0 label_grey: 140087\n","File 269 data_grey: 4 label_grey: 134249\n","File 274 data_grey: 1 label_grey: 148572\n","File 282 data_grey: 0 label_grey: 146469\n","File 284 data_grey: 13 label_grey: 118964\n","File 287 data_grey: 6 label_grey: 101457\n","File 297 data_grey: 6 label_grey: 121332\n","File 302 data_grey: 0 label_grey: 147477\n","File 305 data_grey: 1 label_grey: 84198\n","File 312 data_grey: 3 label_grey: 128855\n","File 313 data_grey: 2 label_grey: 136610\n","File 321 data_grey: 2 label_grey: 143976\n","File 323 data_grey: 0 label_grey: 88434\n","File 326 data_grey: 0 label_grey: 148754\n","File 333 data_grey: 2 label_grey: 138093\n","File 337 data_grey: 1 label_grey: 141514\n","File 344 data_grey: 1 label_grey: 144359\n","File 348 data_grey: 6 label_grey: 132941\n","File 350 data_grey: 1 label_grey: 144704\n","File 352 data_grey: 5 label_grey: 121311\n","File 353 data_grey: 3 label_grey: 138177\n","File 354 data_grey: 0 label_grey: 145985\n","File 355 data_grey: 2 label_grey: 118097\n","File 362 data_grey: 0 label_grey: 148595\n","File 365 data_grey: 0 label_grey: 143599\n","File 371 data_grey: 1 label_grey: 142236\n","File 373 data_grey: 4 label_grey: 117397\n","File 376 data_grey: 1 label_grey: 129686\n","File 381 data_grey: 1 label_grey: 145805\n","File 384 data_grey: 0 label_grey: 148082\n","File 385 data_grey: 0 label_grey: 115088\n","File 420 data_grey: 4 label_grey: 138236\n","File 421 data_grey: 0 label_grey: 102224\n","File 429 data_grey: 11 label_grey: 120821\n","File 445 data_grey: 2 label_grey: 143769\n","File 462 data_grey: 1 label_grey: 137054\n","File 464 data_grey: 1 label_grey: 143020\n","File 466 data_grey: 0 label_grey: 146485\n","File 468 data_grey: 0 label_grey: 147347\n","File 474 data_grey: 1 label_grey: 141301\n","File 475 data_grey: 0 label_grey: 138234\n","File 476 data_grey: 13 label_grey: 120947\n","File 481 data_grey: 1 label_grey: 144113\n","Number of file with grey pixels: 90\n","File 11 data_grey: 4 label_grey: 147579\n","File 16 data_grey: 0 label_grey: 142157\n","File 18 data_grey: 0 label_grey: 143389\n","File 40 data_grey: 0 label_grey: 141449\n","File 57 data_grey: 0 label_grey: 148678\n","File 67 data_grey: 1 label_grey: 129017\n","Number of file with grey pixels: 6\n"]}]},{"cell_type":"code","source":["clean_grey(preclean_test_unseen, 'test_unseen')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0A_R0VoZ1nc7","executionInfo":{"status":"ok","timestamp":1691650825201,"user_tz":240,"elapsed":3065,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"08a1f117-e07f-4abd-ae5b-274a6e81041a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File 11 data_grey: 4 label_grey: 147579\n","File 16 data_grey: 0 label_grey: 142157\n","File 18 data_grey: 0 label_grey: 143389\n","File 40 data_grey: 0 label_grey: 141449\n","File 57 data_grey: 0 label_grey: 148678\n","File 67 data_grey: 1 label_grey: 129017\n","File 75 data_grey: 0 label_grey: 148393\n","File 76 data_grey: 1 label_grey: 142226\n","File 79 data_grey: 1 label_grey: 144768\n","File 83 data_grey: 2 label_grey: 135293\n","File 87 data_grey: 0 label_grey: 144248\n","File 101 data_grey: 2 label_grey: 140036\n","File 103 data_grey: 1 label_grey: 144712\n","File 110 data_grey: 0 label_grey: 142628\n","File 122 data_grey: 1 label_grey: 112381\n","File 129 data_grey: 0 label_grey: 136535\n","File 132 data_grey: 4 label_grey: 132414\n","File 134 data_grey: 1 label_grey: 140372\n","File 135 data_grey: 0 label_grey: 143880\n","File 136 data_grey: 5 label_grey: 141581\n","File 137 data_grey: 0 label_grey: 138225\n","File 138 data_grey: 2 label_grey: 142153\n","File 141 data_grey: 0 label_grey: 144772\n","File 145 data_grey: 0 label_grey: 144965\n","File 147 data_grey: 1 label_grey: 143536\n","File 184 data_grey: 5 label_grey: 126131\n","File 195 data_grey: 0 label_grey: 147684\n","File 197 data_grey: 0 label_grey: 148555\n","File 198 data_grey: 0 label_grey: 145165\n","File 205 data_grey: 1 label_grey: 148045\n","File 206 data_grey: 0 label_grey: 139771\n","File 220 data_grey: 0 label_grey: 143623\n","File 222 data_grey: 0 label_grey: 147682\n","File 223 data_grey: 2 label_grey: 139512\n","File 225 data_grey: 1 label_grey: 147834\n","File 230 data_grey: 1 label_grey: 143895\n","File 234 data_grey: 4 label_grey: 140376\n","File 236 data_grey: 0 label_grey: 146323\n","File 244 data_grey: 0 label_grey: 147885\n","File 245 data_grey: 0 label_grey: 120314\n","File 248 data_grey: 8 label_grey: 113043\n","File 251 data_grey: 1 label_grey: 131568\n","File 258 data_grey: 2 label_grey: 141464\n","File 259 data_grey: 2 label_grey: 130533\n","File 262 data_grey: 0 label_grey: 141920\n","File 263 data_grey: 15 label_grey: 141123\n","File 265 data_grey: 4 label_grey: 139654\n","File 267 data_grey: 0 label_grey: 140087\n","File 269 data_grey: 4 label_grey: 134249\n","File 274 data_grey: 1 label_grey: 148572\n","File 282 data_grey: 0 label_grey: 146469\n","File 284 data_grey: 13 label_grey: 118964\n","File 287 data_grey: 6 label_grey: 101457\n","Number of file with grey pixels: 53\n"]}]},{"cell_type":"markdown","source":["## Eliminate duplicates"],"metadata":{"id":"Rr61Pl2tJkKx"}},{"cell_type":"code","source":["eliminateD_train_unseen = preclean_input_loader('train')\n","eliminateD_test_unseen = preclean_input_loader('test_unseen')"],"metadata":{"id":"k2MRfXpEJ7dI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eliminate_duplicate(train_loader, test_loader, train_name, test_name):\n","  count = 0\n","  num = 0\n","  folder_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/Input'\n","  for data, label in test_loader:\n","    test_data = data\n","    for data1, label1 in train_loader:\n","      train_data = data1\n","      if np.array_equal(test_data, train_data):\n","        dup_direct = folder_path + '/' + test_name + '/data_' + str(count)\n","        if os.path.exists(dup_direct):\n","          for root, dirs, files in os.walk(dup_direct):\n","          # For each file in the directory\n","            for file in files:\n","              # Construct the full path to the file\n","              file_path = os.path.join(root, file)\n","              # Delete the file\n","              os.remove(file_path)\n","              # For each subdirectory in the directory\n","            for dir in dirs:\n","              # Construct the full path to the subdirectory\n","              dir_path = os.path.join(root, dir)\n","              # Delete the subdirectory\n","              os.rmdir(dir_path)\n","          # Delete the top-level directory\n","          os.rmdir(dup_direct)\n","\n","          num += 1\n","    count += 1\n","  print(\"Number of files that are duplicates:\", num)"],"metadata":{"id":"tQmTq3vTKR0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eliminate_duplicate(eliminateD_train_unseen, eliminateD_test_unseen, 'train', 'test_unseen')"],"metadata":{"id":"eW4_UhIhLXbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unseen_test = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/Input/test_unseen'\n","data_dir = os.listdir(unseen_test)\n","print(len(data_dir))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhVhgGD1Ttuy","executionInfo":{"status":"ok","timestamp":1691651975260,"user_tz":240,"elapsed":184,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"19ec67d4-0cde-45b3-bfaf-47d71a661554"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["38\n"]}]},{"cell_type":"markdown","metadata":{"id":"nUrtj7guh-dz"},"source":["## Transfer Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4767,"status":"ok","timestamp":1691651991588,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"},"user_tz":240},"id":"N3FweAcYjSUw","outputId":"96e76494-b36f-4141-8887-dc10b7781b20"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n","100%|██████████| 233M/233M [00:02<00:00, 86.6MB/s]\n"]}],"source":["#@title Pathing\n","alexnet = torchvision.models.alexnet(pretrained=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":535,"status":"ok","timestamp":1690594245428,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"},"user_tz":240},"id":"wkJvL8kawWfR","outputId":"f18b1d32-c3d3-4ba3-d283-f909ca7ef9b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["['data_0', 'data_1', 'data_2', 'data_3', 'data_4', 'data_5', 'data_6', 'data_7', 'data_8', 'data_9', 'data_10', 'data_12', 'data_13', 'data_14', 'data_15', 'data_17', 'data_19', 'data_20', 'data_21', 'data_22', 'data_23', 'data_24', 'data_25', 'data_26', 'data_27', 'data_28', 'data_29', 'data_30', 'data_31', 'data_32', 'data_33', 'data_34', 'data_35', 'data_36', 'data_37', 'data_38', 'data_39', 'data_41', 'data_42', 'data_43', 'data_44', 'data_45', 'data_46', 'data_47', 'data_48', 'data_49', 'data_50', 'data_51', 'data_52', 'data_53', 'data_54', 'data_55', 'data_56', 'data_58', 'data_59', 'data_60', 'data_61', 'data_62', 'data_63', 'data_64', 'data_65', 'data_66', 'data_68', 'data_69', 'data_70', 'data_71', 'data_72', 'data_73', 'data_74', 'data_77', 'data_78', 'data_80', 'data_81', 'data_82', 'data_84', 'data_85', 'data_86', 'data_88', 'data_89', 'data_90', 'data_91', 'data_92', 'data_93', 'data_94', 'data_95', 'data_96', 'data_97', 'data_98', 'data_99', 'data_100', 'data_102', 'data_104', 'data_105', 'data_106', 'data_107', 'data_108', 'data_109', 'data_111', 'data_112', 'data_113', 'data_114', 'data_115', 'data_116', 'data_117', 'data_118', 'data_119', 'data_120', 'data_121', 'data_123', 'data_124', 'data_125', 'data_126', 'data_127', 'data_128', 'data_130', 'data_131', 'data_133', 'data_139', 'data_140', 'data_142', 'data_143', 'data_144', 'data_146', 'data_148', 'data_149', 'data_150', 'data_151', 'data_152', 'data_153', 'data_154', 'data_155', 'data_156', 'data_157', 'data_158', 'data_159', 'data_160', 'data_161', 'data_162', 'data_163', 'data_164', 'data_165', 'data_166', 'data_167', 'data_168', 'data_169', 'data_170', 'data_171', 'data_172', 'data_173', 'data_174', 'data_175', 'data_176', 'data_177', 'data_178', 'data_179', 'data_180', 'data_181', 'data_182', 'data_183', 'data_185', 'data_186', 'data_187', 'data_188', 'data_189', 'data_190', 'data_191', 'data_192', 'data_193', 'data_194', 'data_196', 'data_199', 'data_200', 'data_201', 'data_202', 'data_203', 'data_204', 'data_207', 'data_208', 'data_209', 'data_210', 'data_211', 'data_212', 'data_213', 'data_214', 'data_215', 'data_216', 'data_217', 'data_218', 'data_219', 'data_221', 'data_224', 'data_226', 'data_227', 'data_228', 'data_229', 'data_231', 'data_232', 'data_233', 'data_235', 'data_237', 'data_238', 'data_239', 'data_240', 'data_241', 'data_242', 'data_243', 'data_246', 'data_247', 'data_249', 'data_250', 'data_252', 'data_253', 'data_254', 'data_255', 'data_256', 'data_257', 'data_260', 'data_261', 'data_264', 'data_266', 'data_268', 'data_270', 'data_271', 'data_272', 'data_273', 'data_275', 'data_276', 'data_277', 'data_278', 'data_279', 'data_280', 'data_281', 'data_283', 'data_285', 'data_286', 'data_288', 'data_289', 'data_290', 'data_291', 'data_292', 'data_293', 'data_294', 'data_295', 'data_296', 'data_298', 'data_299', 'data_300', 'data_301', 'data_303', 'data_304', 'data_306', 'data_307', 'data_308', 'data_309', 'data_310', 'data_311', 'data_314', 'data_315', 'data_316', 'data_317', 'data_318', 'data_319', 'data_320', 'data_322', 'data_324', 'data_325', 'data_327', 'data_328', 'data_329', 'data_330', 'data_331', 'data_332', 'data_334', 'data_335', 'data_336', 'data_338', 'data_339', 'data_340', 'data_341', 'data_342', 'data_343', 'data_345', 'data_346', 'data_347', 'data_349', 'data_351', 'data_356', 'data_357', 'data_358', 'data_359', 'data_360', 'data_361', 'data_363', 'data_364', 'data_366', 'data_367', 'data_368', 'data_369', 'data_370', 'data_372', 'data_374', 'data_375', 'data_377', 'data_378', 'data_379', 'data_380', 'data_382', 'data_383', 'data_386', 'data_387', 'data_388', 'data_389', 'data_390', 'data_391', 'data_392', 'data_393', 'data_394', 'data_395', 'data_396', 'data_397', 'data_398', 'data_399', 'data_400', 'data_401', 'data_402', 'data_403', 'data_404', 'data_405', 'data_406', 'data_407', 'data_408', 'data_409', 'data_410', 'data_411', 'data_412', 'data_413', 'data_414', 'data_415', 'data_416', 'data_417', 'data_418', 'data_419', 'data_422', 'data_423', 'data_424', 'data_425', 'data_426', 'data_427', 'data_428', 'data_430', 'data_431', 'data_432', 'data_433', 'data_434', 'data_435', 'data_436', 'data_437', 'data_438', 'data_439', 'data_440', 'data_441', 'data_442', 'data_443', 'data_444', 'data_446', 'data_447', 'data_448', 'data_449', 'data_450', 'data_451', 'data_452', 'data_453', 'data_454', 'data_455', 'data_456', 'data_457', 'data_458', 'data_459', 'data_460', 'data_461', 'data_463', 'data_465', 'data_467', 'data_469', 'data_470', 'data_471', 'data_472', 'data_473', 'data_477', 'data_478', 'data_479', 'data_480', 'data_482']\n","['feature_0.jpg', 'feature_1.jpg', 'feature_2.jpg', 'feature_3.jpg', 'feature_4.jpg', 'feature_5.jpg', 'feature_6.jpg', 'feature_7.jpg', 'feature_8.jpg', 'feature_9.jpg', 'feature_10.jpg', 'feature_11.jpg', 'label.jpg']\n","feature_0\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":22}],"source":["data_path = master_path + '/Input/train'\n","\n","#testing later code\n","data_path\n","item_dir = os.listdir(data_path)\n","print(item_dir)\n","for d in range(len(item_dir)):\n","  data_path2 = os.path.join(data_path, item_dir[d])\n","  feature_path = os.listdir(data_path2)\n","  #print(data_path)\n","print(feature_path)\n","print(feature_path[0][:-4])\n","feature_path[12] == 'label.jpg'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1690594247366,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"},"user_tz":240},"id":"imvNZS_W4ntc","outputId":"7ba29653-d1a4-432a-9f1d-b353a2b03982"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 224, 224])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([256, 6, 6])"]},"metadata":{},"execution_count":23}],"source":["path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/Input/train/data_0/label.jpg'\n","img = Image.open(path)\n","trans_img = np.transpose(img, (2, 1, 0))\n","img_tensor1 = torch.Tensor(trans_img)\n","print(img_tensor1.shape)\n","exp_trans_img = trans_img[np.newaxis, ...]\n","img_tensor = torch.Tensor(trans_img)\n","alexnet_feats = alexnet.features(img_tensor)\n","feats_tensor = torch.from_numpy(alexnet_feats.detach().numpy())\n","feats_tensor.shape\n","sq = feats_tensor.squeeze(0)\n","sq.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":150,"status":"ok","timestamp":1691652051319,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"},"user_tz":240},"id":"SeQemzOCiqnh","outputId":"3a333329-e0e9-4578-a735-3737c2696426"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n1. extract dataset -> prev_masks, good_labels, good_indices, all_input\\n2. concatentate all good features into 1 array:\\n  a. index all inputs with good indices, all_inputs(good_indices[0])\\n3. Outer loop = batch_num\\n4. Inner loop = feature\\n5. Use os to iterate thru the folder\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":59}],"source":["#@title Save alexnet features for each image to folders\n","def alexnet_features_to_folder(set_name, folder_path):\n","    input_path = folder_path + '/Input'\n","    data_path = os.path.join(input_path, set_name)\n","    data_dir = os.listdir(data_path)\n","    for d in data_dir:\n","        #print(d)\n","        data_path1 = os.path.join(data_path, d)\n","        #print(data_path1)\n","        feat_dir = os.listdir(data_path1)\n","        #save each feature to folder with the right name\n","        folder_name = folder_path + \"/Output\" + '/' + set_name + '/' + d\n","        if not os.path.isdir(folder_name):\n","            os.mkdir(folder_name)\n","        #print(feat_dir)\n","        for f in feat_dir:\n","            feat_path = os.path.join(data_path1, f)\n","            img = Image.open(feat_path)\n","            #transpose img to 3x224x224\n","            trans_img = np.transpose(img, (2, 1, 0))\n","            if f != 'label.jpg': #if feature is not the label\n","              exp_trans_img = trans_img[np.newaxis, ...] #convert to 1x3x244x244\n","              #convert img np array to tensor\n","              img_tensor = torch.Tensor(exp_trans_img)\n","              alexnet_feats = alexnet.features(img_tensor)\n","              feats_tensor = torch.from_numpy(alexnet_feats.detach().numpy())\n","              torch.save(feats_tensor.squeeze(0), folder_name + '/' + f[:-4] + '.tensor')\n","            else: #if it is the label, just save the transposed img + don't pass to alexnet\n","              img_tensor = torch.Tensor(exp_trans_img)\n","              torch.save(img_tensor, folder_name + '/' + f[:-4] + '.tensor')\n","\n","'''\n","1. extract dataset -> prev_masks, good_labels, good_indices, all_input\n","2. concatentate all good features into 1 array:\n","  a. index all inputs with good indices, all_inputs(good_indices[0])\n","3. Outer loop = batch_num\n","4. Inner loop = feature\n","5. Use os to iterate thru the folder\n","'''"]},{"cell_type":"code","source":["alexnet_features_to_folder(\"train\", master_path)\n","alexnet_features_to_folder(\"val\", master_path)\n","alexnet_features_to_folder(\"test\", master_path)"],"metadata":{"id":"qTUliZdz4BXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["alexnet_features_to_folder(\"test_unseen\", master_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w-ncE4FA4Fnt","executionInfo":{"status":"ok","timestamp":1691652108412,"user_tz":240,"elapsed":22203,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"4756038a-5211-40c8-ae9d-9d271e03392f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-59-45dfdac2bc4f>:24: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n","  img_tensor = torch.Tensor(exp_trans_img)\n"]}]},{"cell_type":"markdown","metadata":{"id":"i8dncE8O4EpP"},"source":["## Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDrTaLzEivaY","cellView":"form"},"outputs":[],"source":["#@title Load Data\n","\n","def load_embedding(purpose):\n","\n","  output = []\n","\n","  folder_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output'\n","  folder_path = folder_path + '/' + purpose\n","  data_dir = os.listdir(folder_path)\n","\n","  for i in range(len(data_dir)):\n","    data_path = os.path.join(folder_path, data_dir[i])\n","    feature_dir = os.listdir(data_path)\n","\n","    data = []\n","    label = []\n","\n","    for j in range(len(feature_dir)):\n","      feature_path = os.path.join(data_path, feature_dir[j])\n","\n","      print(feature_path)\n","\n","      if j == 0:\n","          data = torch.load(feature_path)\n","          data = data.view(-1, 9216)\n","          data = data[np.newaxis, ...]\n","      elif j >= 1 and j <= 11:\n","          feature = torch.load(feature_path)\n","          feature = feature.view(-1, 9216)\n","          feature = feature[np.newaxis, ...]\n","          data = np.concatenate((data, feature), axis=0)\n","      elif j == 12:\n","          label_path = feature_path[:54] + 'Input/' +feature_path[61:-6] + 'jpg'\n","          label = Image.open(label_path)\n","          label = np.array(label)\n","          label = torch.from_numpy(label)\n","\n","    output.append((data, label))\n","\n","  return output"]},{"cell_type":"code","source":["training_data = load_embedding('train')\n","val_data = load_embedding('val')\n","test_data = load_embedding('test')"],"metadata":{"id":"z5LRRYbA3J-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_unseen_data = load_embedding('test_unseen')"],"metadata":{"id":"6gKF8zya3NvV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691652121548,"user_tz":240,"elapsed":1372,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"8846c309-b042-411a-ee99-652cf098ead0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_242/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_243/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_246/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_247/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_249/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_250/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_252/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_253/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_254/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_255/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_256/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_257/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_260/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_261/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_264/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_266/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_268/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_270/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_271/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_272/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_273/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_275/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_276/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_277/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_278/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_279/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_280/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_281/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_283/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_285/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_286/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_288/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_289/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_290/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_291/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_292/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_293/label.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_0.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_1.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_2.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_3.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_4.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_5.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_6.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_7.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_8.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_9.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_10.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/feature_11.tensor\n","/content/drive/MyDrive/Colab Notebooks/APS360 Project/Output/test_unseen/data_294/label.tensor\n"]}]},{"cell_type":"code","source":["model1 = FirePrediction()\n","out = model1.forward(training_data[9][0])\n","out = np.transpose(out[0,:,:,:],(1,2,0))\n","plt.imshow(out)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"id":"XaeBl4gQ3c1v","executionInfo":{"status":"ok","timestamp":1689296341424,"user_tz":240,"elapsed":269,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"644d58ef-202c-4d44-f178-7166b4daee11"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["torch.save(training_data, '/content/drive/MyDrive/Colab Notebooks/APS360 Project/loaded_training.npy')\n","torch.save(val_data, '/content/drive/MyDrive/Colab Notebooks/APS360 Project/loaded_val.npy')\n","torch.save(test_data, '/content/drive/MyDrive/Colab Notebooks/APS360 Project/loaded_test.npy')"],"metadata":{"id":"t2l_hPFMeoOP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(test_unseen_data, '/content/drive/MyDrive/Colab Notebooks/APS360 Project/loaded_test_unseen.npy')"],"metadata":{"id":"vYb2P0yh3SbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data = torch.load('/content/drive/MyDrive/Colab Notebooks/APS360 Project/loaded_training.npy')\n","val_data = torch.load('/content/drive/MyDrive/Colab Notebooks/APS360 Project/loaded_val.npy')\n","test_data = torch.load('/content/drive/MyDrive/Colab Notebooks/APS360 Project/loaded_test.npy')"],"metadata":{"id":"fSxE8WKtfJkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data = torch.load('/content/drive/MyDrive/Colab Notebooks/APS360 Project/loaded_test_unseen.npy')"],"metadata":{"id":"m8hBBuSi3X2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Load from Offline\n","training_data = torch.load('loaded_training.npy')\n","val_data = torch.load('loaded_val.npy')\n","test_data = torch.load('loaded_test.npy')"],"metadata":{"id":"cSBkTtNQUqj0","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obCbnlfT76p-"},"outputs":[],"source":["#@title Neural Network\n","\n","class FirePrediction(nn.Module):\n","\n","  def __init__(self):\n","    self.name = 'FirePrediction'\n","    super(FirePrediction, self).__init__()\n","    self.rnn = nn.GRU(9216, 9216, 1) #512, 256 smaller hidden size\n","    kernel_size = 4\n","    stride = 2\n","    padding = 1\n","    out_padding = 0\n","    self.decoder = nn.Sequential(\n","        nn.ConvTranspose2d(256, 128, 4, stride, padding, out_padding),\n","        nn.ConvTranspose2d(128, 64, 9, stride, padding, out_padding),\n","        nn.ConvTranspose2d(64, 3, 10, stride, padding, out_padding),\n","    )\n","\n","  def forward(self, x):\n","    x = np.array(x)\n","\n","    if len(x.shape) ==  2:\n","      print(True)\n","      x = np.expand_dims(x, axis=1)\n","\n","    rnn_input = torch.from_numpy(x[0:10, :, :])\n","    h0 = x[11,:,:]\n","    h0 = torch.from_numpy(h0[np.newaxis, ...])\n","\n","    out, h = self.rnn(rnn_input, h0)\n","\n","    output_array = []\n","\n","    for i in range(h.shape[1]):\n","      decoder_input = h[:,i,:]\n","      decoder_input = decoder_input[0,:]\n","      decoder_input = torch.reshape(decoder_input, (256, 6, 6))\n","      decoder_input = decoder_input[np.newaxis, ...]\n","      output = self.decoder(decoder_input)\n","      output = F.relu(output)\n","\n","      if i == 0:\n","        output_array = output\n","      else:\n","        output_array = torch.cat((output_array, output), 0)\n","\n","    return output_array"]},{"cell_type":"code","source":["class UpsampleFirePrediction(nn.Module):\n","\n","    def __init__(self):\n","        self.name = 'UpsampleFirePrediction'\n","        super(UpsampleFirePrediction, self).__init__()\n","        self.rnn = nn.GRU(9216, 9216, 1) #512, 256 smaller hidden size\n","        kernel_size = 4\n","        stride = 2\n","        padding = 1\n","        out_padding = 0\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, 2, stride, padding, out_padding),\n","            nn.Upsample(scale_factor=1.75, mode='bilinear'),\n","            nn.ConvTranspose2d(128, 64, 2, stride, padding, out_padding),\n","            nn.Upsample(scale_factor=1.75, mode='bilinear'),\n","            nn.ConvTranspose2d(64, 3, 4, stride, padding, out_padding),\n","            nn.Upsample(scale_factor=2, mode='bilinear')\n","        )\n","\n","    def forward(self, x):\n","        x = np.array(x)\n","\n","        if len(x.shape) ==  2:\n","          print(True)\n","          x = np.expand_dims(x, axis=1)\n","\n","        rnn_input = torch.from_numpy(x[0:10, :, :])\n","        h0 = x[11,:,:]\n","        h0 = torch.from_numpy(h0[np.newaxis, ...])\n","\n","        out, h = self.rnn(rnn_input, h0)\n","\n","        output_array = []\n","\n","        for i in range(h.shape[1]):\n","            decoder_input = h[:,i,:]\n","            decoder_input = decoder_input[0,:]\n","            decoder_input = torch.reshape(decoder_input, (256, 6, 6))\n","            decoder_input = decoder_input[np.newaxis, ...]\n","            output = self.decoder(decoder_input)\n","            output = F.relu(output)\n","\n","            if i == 0:\n","                output_array = output\n","            else:\n","                output_array = torch.cat((output_array, output), 0)\n","\n","        return output_array"],"metadata":{"id":"2xUVSZYEd1KN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"VyP11NQhMrP9"}},{"cell_type":"code","source":["def drop_last(batch_size, data_loader):\n","    output = data_loader\n","    counter = 1\n","\n","    if batch_size > len(output):\n","        batch_size = len(output)\n","\n","    while len(output) % batch_size != 0:\n","        output = output[:-counter]\n","        counter += 1\n","\n","    return output\n","\n","def concat_batch(batch_size, data_loader):\n","    batched_set = []\n","    for i in range(0, len(data_loader), batch_size):\n","        batched_data = []\n","        batched_label = []\n","        for j in range(0, batch_size, 1):\n","            if j == 0:\n","                batched_data = data_loader[i][0]\n","                label = data_loader[i][1]\n","                label = Image.fromarray(np.array(label))\n","                label = label.resize((64, 64))\n","                label = np.array(label)\n","                batched_label = label[np.newaxis, ...]\n","            else:\n","                batched_data = np.concatenate((batched_data, data_loader[j+i][0]), axis=1)\n","                label = data_loader[j+i][1]\n","                label = Image.fromarray(np.array(label))\n","                label = label.resize((64, 64))\n","                label = np.array(label)\n","                batched_label = np.concatenate((batched_label, label[np.newaxis, ...]), axis=0)\n","            print('batch data shape', batched_data.shape)\n","            print('batch label shape', batched_label.shape)\n","        batched_set.append((batched_data, batched_label))\n","        if i+j == len(data_loader)-1:\n","            break\n","\n","    return batched_set\n","\n","def get_accuracy(model, data_loader, use_cuda=True):\n","    accuracies = []\n","    counter = 0\n","\n","    for features, labels in tqdm(data_loader, desc='Accuracy', unit='batch'):\n","        recon = model(features)\n","        recon = torch.transpose(recon, 1, 2) #give 0 3 1 3\n","        recon = torch.transpose(recon, 2, 3) # give 0 2 3 1\n","        difference = np.array(recon) - np.array(label)\n","        squared_difference = np.square(difference)\n","        mse = np.mean(squared_difference)/(255**2)\n","        accuracies.append(mse)\n","        counter += 1\n","\n","    sigma = 0\n","    for item in accuracies:\n","        sigma += item\n","        #print('sigma is', sigma, 'number of error calculated is', counter, 'the average is', float(sigma)/float(counter))\n","\n","    acc_sum = float(sigma)\n","    item_num = int(counter)\n","\n","    number = (acc_sum, item_num)\n","\n","    return number[1]/number[0]\n","\n","def get_model_name(name, learning_rate, epoch):\n","    path = \"model_{0}_lr{1}_epoch{2}\".format(name, learning_rate, epoch)\n","    return path\n","\n","def get_val_loss(model, valid_loader, criterion):\n","    total_val_loss = 0.0\n","    i = 0\n","\n","    for data,label in tqdm(valid_loader, desc='Loss', unit='batch'):\n","        recon = model(data)\n","\n","        #0 1 2 3\n","        recon = torch.transpose(recon, 1, 2) #give 0 3 1 3\n","        recon = torch.transpose(recon, 2, 3) # give 0 2 3 1\n","        loss = criterion(recon, label.float())\n","        loss.requires_grad=True\n","        total_val_loss += loss.item()\n","        i += 1\n","    val_loss = float(total_val_loss)/i\n","    return val_loss\n","\n","def train(model, train_loader, valid_loader, batch_size=90, num_epochs=5, learning_rate=1e-4, use_cuda=True):\n","    \"\"\" Training loop. You should update this.\"\"\"\n","    torch.manual_seed(42)\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n","\n","    train_err = np.zeros(num_epochs)\n","    train_loss = np.zeros(num_epochs)\n","    val_err = np.zeros(num_epochs)\n","    val_loss = np.zeros(num_epochs)\n","\n","    print('I am loading data into loader')\n","    train_loader = drop_last(batch_size, train_loader)\n","    train_loader = concat_batch(batch_size, train_loader)\n","    valid_loader = drop_last(batch_size, valid_loader)\n","    valid_loader = concat_batch(batch_size, valid_loader)\n","\n","    start_time = time.time()\n","    for epoch in tqdm(range(num_epochs), desc='Epoch', unit='epoch'):\n","        total_train_loss = 0.0\n","        i = 0\n","        for data,label in tqdm(train_loader, desc='Training', unit='batch'):\n","            label = torch.from_numpy(label)\n","\n","            '''\n","            #############################################\n","            # To Enable GPU Usage\n","            if use_cuda and torch.cuda.is_available():\n","                data = data.cuda()\n","                label = label.cuda()\n","            #############################################\n","            '''\n","            recon = model(data)\n","\n","            #0 1 2 3\n","            recon = torch.transpose(recon, 1, 2) #give 0 3 1 3\n","            recon = torch.transpose(recon, 2, 3) # give 0 2 3 1\n","            '''\n","            recon = torch.from_numpy(recon).detach()\n","            label = torch.from_numpy(label).detach()\n","            '''\n","            optimizer.zero_grad()\n","            #print('recon size', recon.size)\n","            #print('label size', label.size, 'label type is', type(label))\n","            loss = criterion(recon, label.float())\n","            #loss.requires_grad=True\n","            total_train_loss += loss.item()\n","            i += 1\n","            loss.backward()\n","            optimizer.step()\n","\n","        print('Calculating epoch', epoch, 'training error')\n","        train_err[epoch] = get_accuracy(model, train_loader)\n","\n","        print('Calculating epoch', epoch, 'training loss')\n","        train_loss[epoch] = float(total_train_loss)/(i + 1)\n","        #print('The shape of train_loader[1][0]:', train_loader[1][0].shape)\n","        #print('The shape of valid_loader[1][0]:', valid_loader[1][0].shape)\n","\n","        print('Calculating epoch', epoch, 'validation error')\n","        val_err[epoch] = get_accuracy(model, valid_loader)\n","\n","        print('Calculating epoch', epoch, 'validation loss')\n","        val_loss[epoch] = get_val_loss(model, valid_loader, criterion)\n","        print((\"Epoch {}: Train acc: {}, Train loss: {} |\"+\n","               \"Validation acc: {}, Validation loss: {}\").format(\n","                   epoch + 1,\n","                   train_err[epoch],\n","                   train_loss[epoch],\n","                   val_err[epoch],\n","                   val_loss[epoch]))\n","        # Save the current model (checkpoint) to a file\n","        model_path = get_model_name(model.name, learning_rate, epoch)\n","        torch.save(model.state_dict(), model_path)\n","    print('Finished Training')\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n","\n","    # Plotting\n","    plt.title(\"Train vs Validation Loss\")\n","    plt.plot(range(1 ,num_epochs+1), train_loss, label=\"Train\")\n","    plt.plot(range(1 ,num_epochs+1), val_loss, label=\"Validation\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Train vs Validation Accuracy\")\n","    plt.plot(range(1 ,num_epochs+1), train_err, label=\"Train\")\n","    plt.plot(range(1 ,num_epochs+1), val_err, label=\"Validation\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Training Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    print(\"Final Training Accuracy: {}\".format(train_err[-1]))\n","    print(\"Final Validation Accuracy: {}\".format(val_err[-1]))"],"metadata":{"id":"ck1tHEoSevmW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = drop_last(90, training_data)\n","train_loader = concat_batch(90, train_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4nG512PXhFc5","executionInfo":{"status":"ok","timestamp":1689894547323,"user_tz":240,"elapsed":1705,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"e303dc7f-91bc-487a-f4e0-9a2296c2802c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["batch data shape (12, 1, 9216)\n","batch label shape (1, 64, 64, 3)\n","batch data shape (12, 2, 9216)\n","batch label shape (2, 64, 64, 3)\n","batch data shape (12, 3, 9216)\n","batch label shape (3, 64, 64, 3)\n","batch data shape (12, 4, 9216)\n","batch label shape (4, 64, 64, 3)\n","batch data shape (12, 5, 9216)\n","batch label shape (5, 64, 64, 3)\n","batch data shape (12, 6, 9216)\n","batch label shape (6, 64, 64, 3)\n","batch data shape (12, 7, 9216)\n","batch label shape (7, 64, 64, 3)\n","batch data shape (12, 8, 9216)\n","batch label shape (8, 64, 64, 3)\n","batch data shape (12, 9, 9216)\n","batch label shape (9, 64, 64, 3)\n","batch data shape (12, 10, 9216)\n","batch label shape (10, 64, 64, 3)\n","batch data shape (12, 11, 9216)\n","batch label shape (11, 64, 64, 3)\n","batch data shape (12, 12, 9216)\n","batch label shape (12, 64, 64, 3)\n","batch data shape (12, 13, 9216)\n","batch label shape (13, 64, 64, 3)\n","batch data shape (12, 14, 9216)\n","batch label shape (14, 64, 64, 3)\n","batch data shape (12, 15, 9216)\n","batch label shape (15, 64, 64, 3)\n","batch data shape (12, 16, 9216)\n","batch label shape (16, 64, 64, 3)\n","batch data shape (12, 17, 9216)\n","batch label shape (17, 64, 64, 3)\n","batch data shape (12, 18, 9216)\n","batch label shape (18, 64, 64, 3)\n","batch data shape (12, 19, 9216)\n","batch label shape (19, 64, 64, 3)\n","batch data shape (12, 20, 9216)\n","batch label shape (20, 64, 64, 3)\n","batch data shape (12, 21, 9216)\n","batch label shape (21, 64, 64, 3)\n","batch data shape (12, 22, 9216)\n","batch label shape (22, 64, 64, 3)\n","batch data shape (12, 23, 9216)\n","batch label shape (23, 64, 64, 3)\n","batch data shape (12, 24, 9216)\n","batch label shape (24, 64, 64, 3)\n","batch data shape (12, 25, 9216)\n","batch label shape (25, 64, 64, 3)\n","batch data shape (12, 26, 9216)\n","batch label shape (26, 64, 64, 3)\n","batch data shape (12, 27, 9216)\n","batch label shape (27, 64, 64, 3)\n","batch data shape (12, 28, 9216)\n","batch label shape (28, 64, 64, 3)\n","batch data shape (12, 29, 9216)\n","batch label shape (29, 64, 64, 3)\n","batch data shape (12, 30, 9216)\n","batch label shape (30, 64, 64, 3)\n","batch data shape (12, 31, 9216)\n","batch label shape (31, 64, 64, 3)\n","batch data shape (12, 32, 9216)\n","batch label shape (32, 64, 64, 3)\n","batch data shape (12, 33, 9216)\n","batch label shape (33, 64, 64, 3)\n","batch data shape (12, 34, 9216)\n","batch label shape (34, 64, 64, 3)\n","batch data shape (12, 35, 9216)\n","batch label shape (35, 64, 64, 3)\n","batch data shape (12, 36, 9216)\n","batch label shape (36, 64, 64, 3)\n","batch data shape (12, 37, 9216)\n","batch label shape (37, 64, 64, 3)\n","batch data shape (12, 38, 9216)\n","batch label shape (38, 64, 64, 3)\n","batch data shape (12, 39, 9216)\n","batch label shape (39, 64, 64, 3)\n","batch data shape (12, 40, 9216)\n","batch label shape (40, 64, 64, 3)\n","batch data shape (12, 41, 9216)\n","batch label shape (41, 64, 64, 3)\n","batch data shape (12, 42, 9216)\n","batch label shape (42, 64, 64, 3)\n","batch data shape (12, 43, 9216)\n","batch label shape (43, 64, 64, 3)\n","batch data shape (12, 44, 9216)\n","batch label shape (44, 64, 64, 3)\n","batch data shape (12, 45, 9216)\n","batch label shape (45, 64, 64, 3)\n","batch data shape (12, 46, 9216)\n","batch label shape (46, 64, 64, 3)\n","batch data shape (12, 47, 9216)\n","batch label shape (47, 64, 64, 3)\n","batch data shape (12, 48, 9216)\n","batch label shape (48, 64, 64, 3)\n","batch data shape (12, 49, 9216)\n","batch label shape (49, 64, 64, 3)\n","batch data shape (12, 50, 9216)\n","batch label shape (50, 64, 64, 3)\n","batch data shape (12, 51, 9216)\n","batch label shape (51, 64, 64, 3)\n","batch data shape (12, 52, 9216)\n","batch label shape (52, 64, 64, 3)\n","batch data shape (12, 53, 9216)\n","batch label shape (53, 64, 64, 3)\n","batch data shape (12, 54, 9216)\n","batch label shape (54, 64, 64, 3)\n","batch data shape (12, 55, 9216)\n","batch label shape (55, 64, 64, 3)\n","batch data shape (12, 56, 9216)\n","batch label shape (56, 64, 64, 3)\n","batch data shape (12, 57, 9216)\n","batch label shape (57, 64, 64, 3)\n","batch data shape (12, 58, 9216)\n","batch label shape (58, 64, 64, 3)\n","batch data shape (12, 59, 9216)\n","batch label shape (59, 64, 64, 3)\n","batch data shape (12, 60, 9216)\n","batch label shape (60, 64, 64, 3)\n","batch data shape (12, 61, 9216)\n","batch label shape (61, 64, 64, 3)\n","batch data shape (12, 62, 9216)\n","batch label shape (62, 64, 64, 3)\n","batch data shape (12, 63, 9216)\n","batch label shape (63, 64, 64, 3)\n","batch data shape (12, 64, 9216)\n","batch label shape (64, 64, 64, 3)\n","batch data shape (12, 65, 9216)\n","batch label shape (65, 64, 64, 3)\n","batch data shape (12, 66, 9216)\n","batch label shape (66, 64, 64, 3)\n","batch data shape (12, 67, 9216)\n","batch label shape (67, 64, 64, 3)\n","batch data shape (12, 68, 9216)\n","batch label shape (68, 64, 64, 3)\n","batch data shape (12, 69, 9216)\n","batch label shape (69, 64, 64, 3)\n","batch data shape (12, 70, 9216)\n","batch label shape (70, 64, 64, 3)\n","batch data shape (12, 71, 9216)\n","batch label shape (71, 64, 64, 3)\n","batch data shape (12, 72, 9216)\n","batch label shape (72, 64, 64, 3)\n","batch data shape (12, 73, 9216)\n","batch label shape (73, 64, 64, 3)\n","batch data shape (12, 74, 9216)\n","batch label shape (74, 64, 64, 3)\n","batch data shape (12, 75, 9216)\n","batch label shape (75, 64, 64, 3)\n","batch data shape (12, 76, 9216)\n","batch label shape (76, 64, 64, 3)\n","batch data shape (12, 77, 9216)\n","batch label shape (77, 64, 64, 3)\n","batch data shape (12, 78, 9216)\n","batch label shape (78, 64, 64, 3)\n","batch data shape (12, 79, 9216)\n","batch label shape (79, 64, 64, 3)\n","batch data shape (12, 80, 9216)\n","batch label shape (80, 64, 64, 3)\n","batch data shape (12, 81, 9216)\n","batch label shape (81, 64, 64, 3)\n","batch data shape (12, 82, 9216)\n","batch label shape (82, 64, 64, 3)\n","batch data shape (12, 83, 9216)\n","batch label shape (83, 64, 64, 3)\n","batch data shape (12, 84, 9216)\n","batch label shape (84, 64, 64, 3)\n","batch data shape (12, 85, 9216)\n","batch label shape (85, 64, 64, 3)\n","batch data shape (12, 86, 9216)\n","batch label shape (86, 64, 64, 3)\n","batch data shape (12, 87, 9216)\n","batch label shape (87, 64, 64, 3)\n","batch data shape (12, 88, 9216)\n","batch label shape (88, 64, 64, 3)\n","batch data shape (12, 89, 9216)\n","batch label shape (89, 64, 64, 3)\n","batch data shape (12, 90, 9216)\n","batch label shape (90, 64, 64, 3)\n","batch data shape (12, 1, 9216)\n","batch label shape (1, 64, 64, 3)\n","batch data shape (12, 2, 9216)\n","batch label shape (2, 64, 64, 3)\n","batch data shape (12, 3, 9216)\n","batch label shape (3, 64, 64, 3)\n","batch data shape (12, 4, 9216)\n","batch label shape (4, 64, 64, 3)\n","batch data shape (12, 5, 9216)\n","batch label shape (5, 64, 64, 3)\n","batch data shape (12, 6, 9216)\n","batch label shape (6, 64, 64, 3)\n","batch data shape (12, 7, 9216)\n","batch label shape (7, 64, 64, 3)\n","batch data shape (12, 8, 9216)\n","batch label shape (8, 64, 64, 3)\n","batch data shape (12, 9, 9216)\n","batch label shape (9, 64, 64, 3)\n","batch data shape (12, 10, 9216)\n","batch label shape (10, 64, 64, 3)\n","batch data shape (12, 11, 9216)\n","batch label shape (11, 64, 64, 3)\n","batch data shape (12, 12, 9216)\n","batch label shape (12, 64, 64, 3)\n","batch data shape (12, 13, 9216)\n","batch label shape (13, 64, 64, 3)\n","batch data shape (12, 14, 9216)\n","batch label shape (14, 64, 64, 3)\n","batch data shape (12, 15, 9216)\n","batch label shape (15, 64, 64, 3)\n","batch data shape (12, 16, 9216)\n","batch label shape (16, 64, 64, 3)\n","batch data shape (12, 17, 9216)\n","batch label shape (17, 64, 64, 3)\n","batch data shape (12, 18, 9216)\n","batch label shape (18, 64, 64, 3)\n","batch data shape (12, 19, 9216)\n","batch label shape (19, 64, 64, 3)\n","batch data shape (12, 20, 9216)\n","batch label shape (20, 64, 64, 3)\n","batch data shape (12, 21, 9216)\n","batch label shape (21, 64, 64, 3)\n","batch data shape (12, 22, 9216)\n","batch label shape (22, 64, 64, 3)\n","batch data shape (12, 23, 9216)\n","batch label shape (23, 64, 64, 3)\n","batch data shape (12, 24, 9216)\n","batch label shape (24, 64, 64, 3)\n","batch data shape (12, 25, 9216)\n","batch label shape (25, 64, 64, 3)\n","batch data shape (12, 26, 9216)\n","batch label shape (26, 64, 64, 3)\n","batch data shape (12, 27, 9216)\n","batch label shape (27, 64, 64, 3)\n","batch data shape (12, 28, 9216)\n","batch label shape (28, 64, 64, 3)\n","batch data shape (12, 29, 9216)\n","batch label shape (29, 64, 64, 3)\n","batch data shape (12, 30, 9216)\n","batch label shape (30, 64, 64, 3)\n","batch data shape (12, 31, 9216)\n","batch label shape (31, 64, 64, 3)\n","batch data shape (12, 32, 9216)\n","batch label shape (32, 64, 64, 3)\n","batch data shape (12, 33, 9216)\n","batch label shape (33, 64, 64, 3)\n","batch data shape (12, 34, 9216)\n","batch label shape (34, 64, 64, 3)\n","batch data shape (12, 35, 9216)\n","batch label shape (35, 64, 64, 3)\n","batch data shape (12, 36, 9216)\n","batch label shape (36, 64, 64, 3)\n","batch data shape (12, 37, 9216)\n","batch label shape (37, 64, 64, 3)\n","batch data shape (12, 38, 9216)\n","batch label shape (38, 64, 64, 3)\n","batch data shape (12, 39, 9216)\n","batch label shape (39, 64, 64, 3)\n","batch data shape (12, 40, 9216)\n","batch label shape (40, 64, 64, 3)\n","batch data shape (12, 41, 9216)\n","batch label shape (41, 64, 64, 3)\n","batch data shape (12, 42, 9216)\n","batch label shape (42, 64, 64, 3)\n","batch data shape (12, 43, 9216)\n","batch label shape (43, 64, 64, 3)\n","batch data shape (12, 44, 9216)\n","batch label shape (44, 64, 64, 3)\n","batch data shape (12, 45, 9216)\n","batch label shape (45, 64, 64, 3)\n","batch data shape (12, 46, 9216)\n","batch label shape (46, 64, 64, 3)\n","batch data shape (12, 47, 9216)\n","batch label shape (47, 64, 64, 3)\n","batch data shape (12, 48, 9216)\n","batch label shape (48, 64, 64, 3)\n","batch data shape (12, 49, 9216)\n","batch label shape (49, 64, 64, 3)\n","batch data shape (12, 50, 9216)\n","batch label shape (50, 64, 64, 3)\n","batch data shape (12, 51, 9216)\n","batch label shape (51, 64, 64, 3)\n","batch data shape (12, 52, 9216)\n","batch label shape (52, 64, 64, 3)\n","batch data shape (12, 53, 9216)\n","batch label shape (53, 64, 64, 3)\n","batch data shape (12, 54, 9216)\n","batch label shape (54, 64, 64, 3)\n","batch data shape (12, 55, 9216)\n","batch label shape (55, 64, 64, 3)\n","batch data shape (12, 56, 9216)\n","batch label shape (56, 64, 64, 3)\n","batch data shape (12, 57, 9216)\n","batch label shape (57, 64, 64, 3)\n","batch data shape (12, 58, 9216)\n","batch label shape (58, 64, 64, 3)\n","batch data shape (12, 59, 9216)\n","batch label shape (59, 64, 64, 3)\n","batch data shape (12, 60, 9216)\n","batch label shape (60, 64, 64, 3)\n","batch data shape (12, 61, 9216)\n","batch label shape (61, 64, 64, 3)\n","batch data shape (12, 62, 9216)\n","batch label shape (62, 64, 64, 3)\n","batch data shape (12, 63, 9216)\n","batch label shape (63, 64, 64, 3)\n","batch data shape (12, 64, 9216)\n","batch label shape (64, 64, 64, 3)\n","batch data shape (12, 65, 9216)\n","batch label shape (65, 64, 64, 3)\n","batch data shape (12, 66, 9216)\n","batch label shape (66, 64, 64, 3)\n","batch data shape (12, 67, 9216)\n","batch label shape (67, 64, 64, 3)\n","batch data shape (12, 68, 9216)\n","batch label shape (68, 64, 64, 3)\n","batch data shape (12, 69, 9216)\n","batch label shape (69, 64, 64, 3)\n","batch data shape (12, 70, 9216)\n","batch label shape (70, 64, 64, 3)\n","batch data shape (12, 71, 9216)\n","batch label shape (71, 64, 64, 3)\n","batch data shape (12, 72, 9216)\n","batch label shape (72, 64, 64, 3)\n","batch data shape (12, 73, 9216)\n","batch label shape (73, 64, 64, 3)\n","batch data shape (12, 74, 9216)\n","batch label shape (74, 64, 64, 3)\n","batch data shape (12, 75, 9216)\n","batch label shape (75, 64, 64, 3)\n","batch data shape (12, 76, 9216)\n","batch label shape (76, 64, 64, 3)\n","batch data shape (12, 77, 9216)\n","batch label shape (77, 64, 64, 3)\n","batch data shape (12, 78, 9216)\n","batch label shape (78, 64, 64, 3)\n","batch data shape (12, 79, 9216)\n","batch label shape (79, 64, 64, 3)\n","batch data shape (12, 80, 9216)\n","batch label shape (80, 64, 64, 3)\n","batch data shape (12, 81, 9216)\n","batch label shape (81, 64, 64, 3)\n","batch data shape (12, 82, 9216)\n","batch label shape (82, 64, 64, 3)\n","batch data shape (12, 83, 9216)\n","batch label shape (83, 64, 64, 3)\n","batch data shape (12, 84, 9216)\n","batch label shape (84, 64, 64, 3)\n","batch data shape (12, 85, 9216)\n","batch label shape (85, 64, 64, 3)\n","batch data shape (12, 86, 9216)\n","batch label shape (86, 64, 64, 3)\n","batch data shape (12, 87, 9216)\n","batch label shape (87, 64, 64, 3)\n","batch data shape (12, 88, 9216)\n","batch label shape (88, 64, 64, 3)\n","batch data shape (12, 89, 9216)\n","batch label shape (89, 64, 64, 3)\n","batch data shape (12, 90, 9216)\n","batch label shape (90, 64, 64, 3)\n","batch data shape (12, 1, 9216)\n","batch label shape (1, 64, 64, 3)\n","batch data shape (12, 2, 9216)\n","batch label shape (2, 64, 64, 3)\n","batch data shape (12, 3, 9216)\n","batch label shape (3, 64, 64, 3)\n","batch data shape (12, 4, 9216)\n","batch label shape (4, 64, 64, 3)\n","batch data shape (12, 5, 9216)\n","batch label shape (5, 64, 64, 3)\n","batch data shape (12, 6, 9216)\n","batch label shape (6, 64, 64, 3)\n","batch data shape (12, 7, 9216)\n","batch label shape (7, 64, 64, 3)\n","batch data shape (12, 8, 9216)\n","batch label shape (8, 64, 64, 3)\n","batch data shape (12, 9, 9216)\n","batch label shape (9, 64, 64, 3)\n","batch data shape (12, 10, 9216)\n","batch label shape (10, 64, 64, 3)\n","batch data shape (12, 11, 9216)\n","batch label shape (11, 64, 64, 3)\n","batch data shape (12, 12, 9216)\n","batch label shape (12, 64, 64, 3)\n","batch data shape (12, 13, 9216)\n","batch label shape (13, 64, 64, 3)\n","batch data shape (12, 14, 9216)\n","batch label shape (14, 64, 64, 3)\n","batch data shape (12, 15, 9216)\n","batch label shape (15, 64, 64, 3)\n","batch data shape (12, 16, 9216)\n","batch label shape (16, 64, 64, 3)\n","batch data shape (12, 17, 9216)\n","batch label shape (17, 64, 64, 3)\n","batch data shape (12, 18, 9216)\n","batch label shape (18, 64, 64, 3)\n","batch data shape (12, 19, 9216)\n","batch label shape (19, 64, 64, 3)\n","batch data shape (12, 20, 9216)\n","batch label shape (20, 64, 64, 3)\n","batch data shape (12, 21, 9216)\n","batch label shape (21, 64, 64, 3)\n","batch data shape (12, 22, 9216)\n","batch label shape (22, 64, 64, 3)\n","batch data shape (12, 23, 9216)\n","batch label shape (23, 64, 64, 3)\n","batch data shape (12, 24, 9216)\n","batch label shape (24, 64, 64, 3)\n","batch data shape (12, 25, 9216)\n","batch label shape (25, 64, 64, 3)\n","batch data shape (12, 26, 9216)\n","batch label shape (26, 64, 64, 3)\n","batch data shape (12, 27, 9216)\n","batch label shape (27, 64, 64, 3)\n","batch data shape (12, 28, 9216)\n","batch label shape (28, 64, 64, 3)\n","batch data shape (12, 29, 9216)\n","batch label shape (29, 64, 64, 3)\n","batch data shape (12, 30, 9216)\n","batch label shape (30, 64, 64, 3)\n","batch data shape (12, 31, 9216)\n","batch label shape (31, 64, 64, 3)\n","batch data shape (12, 32, 9216)\n","batch label shape (32, 64, 64, 3)\n","batch data shape (12, 33, 9216)\n","batch label shape (33, 64, 64, 3)\n","batch data shape (12, 34, 9216)\n","batch label shape (34, 64, 64, 3)\n","batch data shape (12, 35, 9216)\n","batch label shape (35, 64, 64, 3)\n","batch data shape (12, 36, 9216)\n","batch label shape (36, 64, 64, 3)\n","batch data shape (12, 37, 9216)\n","batch label shape (37, 64, 64, 3)\n","batch data shape (12, 38, 9216)\n","batch label shape (38, 64, 64, 3)\n","batch data shape (12, 39, 9216)\n","batch label shape (39, 64, 64, 3)\n","batch data shape (12, 40, 9216)\n","batch label shape (40, 64, 64, 3)\n","batch data shape (12, 41, 9216)\n","batch label shape (41, 64, 64, 3)\n","batch data shape (12, 42, 9216)\n","batch label shape (42, 64, 64, 3)\n","batch data shape (12, 43, 9216)\n","batch label shape (43, 64, 64, 3)\n","batch data shape (12, 44, 9216)\n","batch label shape (44, 64, 64, 3)\n","batch data shape (12, 45, 9216)\n","batch label shape (45, 64, 64, 3)\n","batch data shape (12, 46, 9216)\n","batch label shape (46, 64, 64, 3)\n","batch data shape (12, 47, 9216)\n","batch label shape (47, 64, 64, 3)\n","batch data shape (12, 48, 9216)\n","batch label shape (48, 64, 64, 3)\n","batch data shape (12, 49, 9216)\n","batch label shape (49, 64, 64, 3)\n","batch data shape (12, 50, 9216)\n","batch label shape (50, 64, 64, 3)\n","batch data shape (12, 51, 9216)\n","batch label shape (51, 64, 64, 3)\n","batch data shape (12, 52, 9216)\n","batch label shape (52, 64, 64, 3)\n","batch data shape (12, 53, 9216)\n","batch label shape (53, 64, 64, 3)\n","batch data shape (12, 54, 9216)\n","batch label shape (54, 64, 64, 3)\n","batch data shape (12, 55, 9216)\n","batch label shape (55, 64, 64, 3)\n","batch data shape (12, 56, 9216)\n","batch label shape (56, 64, 64, 3)\n","batch data shape (12, 57, 9216)\n","batch label shape (57, 64, 64, 3)\n","batch data shape (12, 58, 9216)\n","batch label shape (58, 64, 64, 3)\n","batch data shape (12, 59, 9216)\n","batch label shape (59, 64, 64, 3)\n","batch data shape (12, 60, 9216)\n","batch label shape (60, 64, 64, 3)\n","batch data shape (12, 61, 9216)\n","batch label shape (61, 64, 64, 3)\n","batch data shape (12, 62, 9216)\n","batch label shape (62, 64, 64, 3)\n","batch data shape (12, 63, 9216)\n","batch label shape (63, 64, 64, 3)\n","batch data shape (12, 64, 9216)\n","batch label shape (64, 64, 64, 3)\n","batch data shape (12, 65, 9216)\n","batch label shape (65, 64, 64, 3)\n","batch data shape (12, 66, 9216)\n","batch label shape (66, 64, 64, 3)\n","batch data shape (12, 67, 9216)\n","batch label shape (67, 64, 64, 3)\n","batch data shape (12, 68, 9216)\n","batch label shape (68, 64, 64, 3)\n","batch data shape (12, 69, 9216)\n","batch label shape (69, 64, 64, 3)\n","batch data shape (12, 70, 9216)\n","batch label shape (70, 64, 64, 3)\n","batch data shape (12, 71, 9216)\n","batch label shape (71, 64, 64, 3)\n","batch data shape (12, 72, 9216)\n","batch label shape (72, 64, 64, 3)\n","batch data shape (12, 73, 9216)\n","batch label shape (73, 64, 64, 3)\n","batch data shape (12, 74, 9216)\n","batch label shape (74, 64, 64, 3)\n","batch data shape (12, 75, 9216)\n","batch label shape (75, 64, 64, 3)\n","batch data shape (12, 76, 9216)\n","batch label shape (76, 64, 64, 3)\n","batch data shape (12, 77, 9216)\n","batch label shape (77, 64, 64, 3)\n","batch data shape (12, 78, 9216)\n","batch label shape (78, 64, 64, 3)\n","batch data shape (12, 79, 9216)\n","batch label shape (79, 64, 64, 3)\n","batch data shape (12, 80, 9216)\n","batch label shape (80, 64, 64, 3)\n","batch data shape (12, 81, 9216)\n","batch label shape (81, 64, 64, 3)\n","batch data shape (12, 82, 9216)\n","batch label shape (82, 64, 64, 3)\n","batch data shape (12, 83, 9216)\n","batch label shape (83, 64, 64, 3)\n","batch data shape (12, 84, 9216)\n","batch label shape (84, 64, 64, 3)\n","batch data shape (12, 85, 9216)\n","batch label shape (85, 64, 64, 3)\n","batch data shape (12, 86, 9216)\n","batch label shape (86, 64, 64, 3)\n","batch data shape (12, 87, 9216)\n","batch label shape (87, 64, 64, 3)\n","batch data shape (12, 88, 9216)\n","batch label shape (88, 64, 64, 3)\n","batch data shape (12, 89, 9216)\n","batch label shape (89, 64, 64, 3)\n","batch data shape (12, 90, 9216)\n","batch label shape (90, 64, 64, 3)\n"]}]},{"cell_type":"code","source":["print(len(train_loader))\n","print(len(train_loader[0]))\n","print(train_loader[0][0].shape)\n","print(train_loader[0][1].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_u_uNdWshF41","executionInfo":{"status":"ok","timestamp":1689894548870,"user_tz":240,"elapsed":119,"user":{"displayName":"Harry Zhou","userId":"07056208126900722723"}},"outputId":"160f81d4-bcda-4f13-9e7a-fb478da9c2b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","2\n","(12, 90, 9216)\n","(90, 64, 64, 3)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQWdtu5a_FyX"},"outputs":[],"source":["print('Calculating epoch', epoch, 'training error')"]},{"cell_type":"code","source":["model = FirePrediction()\n","train(model, training_data, val_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VAtAZ6y57PX7","outputId":"fc4f04be-b662-4122-f1bb-33dae91ac7e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I am loading data into loader\n"]},{"output_type":"stream","name":"stderr","text":["Epoch:   0%|          | 0/5 [00:00<?, ?epoch/s]\n","Training:   0%|          | 0/269 [00:00<?, ?batch/s]\u001b[A"]}]},{"cell_type":"code","source":["print(model.rnn.weight.grad)"],"metadata":{"id":"CytQBUYQAwvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path2 = '/content/model_FirePrediction_lr0.0001_epoch1'\n","net2 = torch.load(path2)\n","net2"],"metadata":{"id":"GZsUEm-rwi-b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/model_FirePrediction_lr0.0001_epoch2'\n","net = torch.load(path)\n","net"],"metadata":{"id":"BPCqYJDqu64w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_5 = UpsampleFirePrediction()\n","train(model_5, train_data, val_data, num_epochs=300, batch_size=128, learning_rate=0.0001)"],"metadata":{"id":"7RTZ3yJQpCbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to view predictions\n","def image_viewer(image_array):\n","    if len(image_array.shape) == 4:\n","        image_array = image_array[0,:,:,:]\n","        image_array = np.array(image_array.detach().numpy())\n","        image_array = np.transpose(image_array, (1, 2, 0))\n","    elif len(image_array.shape) == 3:\n","        image_array = np.array(image_array.detach().numpy())\n","        #image_array = np.transpose(image_array, (0, =))\n","    else:\n","        raise Typererror('Incorrect dimension')\n","\n","    plt.imshow(image_array)\n","    plt.show()\n","    return"],"metadata":{"id":"Wc7jPHGzotpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the unseen test data results\n","for i in range(len(test_unseen_data)):\n","    print('-----' + str(i) +'th testing data' + '-----')\n","    output = model_5(test_unseen_data[i][0])\n","    label = test_unseen_data[i][1]\n","    image_viewer(output)\n","    image_viewer(label)\n","    if i > len(test_unseen_data):\n","        break"],"metadata":{"id":"_9ecYgyrovf2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline\n","\n"],"metadata":{"id":"tB6COyyfMyfo"}},{"cell_type":"code","source":["#@title Load Input for Baseline\n","\n","def baseline_input_loader(purpose):\n","\n","  output = []\n","\n","  folder_path = '/content/drive/MyDrive/Colab Notebooks/APS360 Project/Input'\n","  folder_path = folder_path + '/' + purpose\n","  data_dir = os.listdir(folder_path)\n","\n","  for i in range(len(data_dir)):\n","    data_path = os.path.join(folder_path, data_dir[i])\n","    feature_dir =  os.listdir(data_path)\n","\n","    data = []\n","    label = []\n","\n","    for j in range(len(feature_dir)):\n","      feature_path = os.path.join(data_path, feature_dir[j])\n","\n","      print(feature_path)\n","\n","      if j == 11:\n","          data = Image.open(feature_path)\n","          data = np.array(data)\n","          '''\n","          elif j >= 1 and j <= 11:\n","              feature = torch.load(feature_path)\n","              feature = feature.view(-1, 9216)\n","              feature = feature[np.newaxis, ...]\n","              data = np.concatenate((data, feature), axis=0)\n","          '''\n","      elif j == 12:\n","          #label_path = feature_path[:54] + 'Input/' +feature_path[61:-6] + 'jpg'\n","          label_path = feature_path\n","          label = Image.open(label_path)\n","          label = np.array(label)\n","\n","    output.append((data, label))\n","\n","  return output"],"metadata":{"id":"hBDUVJXhT57w","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_baseline = baseline_input_loader('train')\n","val_baseline = baseline_input_loader('val')\n","testing_baseline = baseline_input_loader('test')"],"metadata":{"id":"yOM0qltjUm6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_unseen_baseline = baseline_input_loader('test_unseen')"],"metadata":{"id":"JUztp4CzeXHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(training_baseline)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ZJQyft0W0vu","executionInfo":{"status":"ok","timestamp":1691100728140,"user_tz":240,"elapsed":292,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"4a2201ff-a669-4ca8-ad7e-028857306dcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["393"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["def avg_red(dataset):\n","  red = 0\n","  count = 0\n","  for data, label in dataset:\n","    red += np.count_nonzero(data == 255)\n","    count += 1\n","\n","  return red/count\n","\n","def pred_correct(data, label, average):\n","  red_data = np.count_nonzero(data == 255)\n","  red_label = np.count_nonzero(label == 255)\n","  if (red_data < average) and (red_label < red_data):\n","      return True\n","  elif (red_data > average) and (red_label > red_data):\n","      return True\n","  elif (red_data == average) and (red_label == red_data):\n","      return True\n","  else:\n","      return False"],"metadata":{"id":"1nenUq0Bw8qz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = []\n","average_red = avg_red(training_baseline)\n","for data, label in training_baseline:\n","    pred = pred_correct(data, label, average_red)\n","    prediction.append(pred)\n","\n","print(\"Number of correct predictions:\", sum(prediction))\n","print(\"Number of incorrect predictions:\", len(prediction)-sum(prediction))\n","\n","print(\"The training accuracy of our baseline model is\", sum(prediction)/len(prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4-spZDbz6Ru","executionInfo":{"status":"ok","timestamp":1691100735408,"user_tz":240,"elapsed":272,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"bca4b758-caea-4c86-dcf1-1747d63538b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of correct predictions: 131\n","Number of incorrect predictions: 262\n","The training accuracy of our baseline model is 0.3333333333333333\n"]}]},{"cell_type":"code","source":["prediction = []\n","average_red = avg_red(val_baseline)\n","for data, label in val_baseline:\n","    pred = pred_correct(data, label, average_red)\n","    prediction.append(pred)\n","\n","print(\"Number of correct predictions:\", sum(prediction))\n","print(\"Number of incorrect predictions:\", len(prediction)-sum(prediction))\n","\n","print(\"The validation accuracy of our baseline model is\", sum(prediction)/len(prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YcdJff_MehYZ","executionInfo":{"status":"ok","timestamp":1691101107029,"user_tz":240,"elapsed":275,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"3d1257e8-6d6a-4b89-d325-ef1d929069ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of correct predictions: 23\n","Number of incorrect predictions: 44\n","The validation accuracy of our baseline model is 0.34328358208955223\n"]}]},{"cell_type":"code","source":["prediction = []\n","average_red = avg_red(testing_baseline)\n","for data, label in testing_baseline:\n","    pred = pred_correct(data, label, average_red)\n","    prediction.append(pred)\n","\n","print(\"Number of correct predictions:\", sum(prediction))\n","print(\"Number of incorrect predictions:\", len(prediction)-sum(prediction))\n","\n","print(\"The test accuracy of our baseline model is\", sum(prediction)/len(prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjhJzt2TdWVf","executionInfo":{"status":"ok","timestamp":1691100907127,"user_tz":240,"elapsed":265,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"fdf68ff2-e180-48d8-b1c8-a03d81074465"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of correct predictions: 13\n","Number of incorrect predictions: 49\n","The test accuracy of our baseline model is 0.20967741935483872\n"]}]},{"cell_type":"code","source":["prediction = []\n","average_red = avg_red(test_unseen_baseline)\n","for data, label in test_unseen_baseline:\n","    pred = pred_correct(data, label, average_red)\n","    prediction.append(pred)\n","\n","print(\"Number of correct predictions:\", sum(prediction))\n","print(\"Number of incorrect predictions:\", len(prediction)-sum(prediction))\n","\n","print(\"The accuracy of our baseline model on an unseen test dataset is\", sum(prediction)/len(prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PWDp399AeAP_","executionInfo":{"status":"ok","timestamp":1691101101458,"user_tz":240,"elapsed":314,"user":{"displayName":"Audrey Tian","userId":"17138929938439060499"}},"outputId":"74ece403-6d57-45ef-e043-2e385b7974a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of correct predictions: 28\n","Number of incorrect predictions: 43\n","The accuracy of our baseline model on an unseen test dataset is 0.39436619718309857\n"]}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["nUrtj7guh-dz","VyP11NQhMrP9"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}